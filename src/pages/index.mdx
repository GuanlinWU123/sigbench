---
layout: ../layouts/Layout.astro
title: >-
    Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
favicon: favicon.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"

import benchmark_overview from "../assets/benchmark_overview.png"
import evaluation_metrics from "../assets/evaluation_metrics.png"
import annotation_pipeline from "../assets/annotation_pipeline.png"
import zs_results_grid from "../assets/zs_results_grid.png"
import icl_results_grid from "../assets/icl_results_grid.png"
import icl_vis_grid from "../assets/icl_vis_grid.png"
import human_like_results from "../assets/human_like_results.png"

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Guanlin Wu",
      url: "https://guanlinwu123.github.io",
      notes: ["*"],
    },
    {
      name: "Boyan Su",
      url: "https://www.linkedin.com/in/boyan-su-03978126a/",
      notes: ["*"],
    },
    {
      name: "Yang Zhao",
      url: "https://www.linkedin.com/in/yang-zhao-b60b70273/",
    },
    {
      name: "Pu Wang",
      url: "https://www.linkedin.com/in/pu-wang-662a59300/",
    },
    {
      name: "Yichen Lin",
    },
    {
      name: "Hao Frank Yang",
      url: "http://haofrankyang.net",
      notes: ["†"],
    }
  ]}
  conference="NeurIPS 2025 (Spotlight)"
  institution="Johns Hopkins University"
  notes={[
    {
      symbol: "*",
      text: "Equal Contribution",
    },
    {
      symbol: "†",
      text: "Corresponding Author",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://github.com/RomanHauksson/academic-project-astro-template",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    },
    {
      name: "Hugging Face",
      url: "https://huggingface.co/datasets/Frank0930/SIGBench",
      icon: "simple-icons:huggingface"
    }
  ]}
  />

{/* <Video source={outside} /> */}

<HighlightedSection>

## Abstract

How to encode visual-spatial intelligence (VSI) into representative and informative features remains an open challenge. Instead of representing VSI through Visual Question Answering (VQA)-style solely, we introduce spatial intelligence grid (SIG): a structured, grid-based data schema that embeds geometrical spatial relationship among objects along with physical priors in human world, as a complementary representation. We further derive a set of SIG-informed evaluation metrics that rigorously quantify a model’s true VSI capabilities. In few-shot in-context learning experiments on state-of-the-art multimodal LLMs (e.g. GPT-4o, Gemini-2.5-Pro), SIG yields consistently larger, more stable, and more comprehensive improvements across all VSI metrics compared to VQA-style representations, demonstrating its potential as a novel data schema for learning VSI. Based on SIG, we create SIGBench, a benchmark containing 1.4K driving frames annotated with ground-truth SIG labels and human gaze attention, supporting both grid-based machine VSI tasks and human-like attention-driven VSI tasks in autonomous-driving scenarios.

</HighlightedSection>

## Highlight

<Figure>
  <Image slot="figure" source={benchmark_overview} altText="benchmark_overview." />
  <span slot="caption">Overview of Human-like SIG in AD Scenario. In the left, we use SIG to represent the spatial relation of traffic sign, traffic lights, vehicles and self (ego-vehicle) in the image. We apply homographic transformation to convert human gaze attention from image to SIG size in the right. Combining them, we get human-like SIG, which can then be extracted to human-like SRG and SRP in middle part. The order denotes the rank of an object of the same category in the image from left to right (e.g. black truck 1 is the left-most object among vehicles).</span>
</Figure>

### Evaluation Metrics
Based on SIG, we can extract a directed spatial relation graph (SRG) that describes the spatial relation (direction+distance in grid) of each object and spatial relation paragraph (SRP) that describes spatial relation of each object within a text manner. To quantitatively assess the a model’s VSI, we propose three novel evaluation metrics: Multi-level spatial matching (MLSM), spatial relation graph similarity (SRGS) and semantic relational distance (SRD). MLSM compares object positions directly within the SIG representation, capturing absolute localization accuracy. SRGS measures both node-wise and edge-wise correspondence between predicted and ground-truth (GT) SRG, emphasizing relation classification and structure. SRD computes a semantic relational distance between predicted and ground-truth prepositions in SRP, evaluating the fidelity of both directional and proximal relations.

<Figure>
  <Image slot="figure" source={evaluation_metrics} altText="evaluation_metrics." />
  <span slot="caption">Illustration Examples of MLSM and SRGS. At the start of both MLSM and SRGS, it will match the objects between the predicted and GT SIG using bipartite matching. For MLSM, we provide example for calculating TP, FP and FN for vehicles in the boxed area in upper part. For SRGS, we highlight the node and edge needed to insert and substitute and their total edit distance in lower part.</span>
</Figure>

## SIGBench Dataset
We introduce SIGBench, a benchmark for quantifying both grid-based and human-like VSI in MLLMs within AD scenario. SIGBench comprises 1,423 frames, each annotated with (i) SIG and human-like SIG, (ii) SRP and human-like SRP and (iii) a gaze attention map in image size. SIGBench contains two main task clusters: grid-based VSI tasks: spatial intelligence grid creation (SIGC) and spatial relation paragraph filling (SRPF) and human-like VSI tasks: human-like SIGC and SRPF, and gaze prediction.
<Figure>
  <Image slot="figure" source={annotation_pipeline} altText="annotation_pipeline." />
  <span slot="caption">(a) is the annotation pipeline of SIGBench and (b) illustrates the SIGC and SRPF tasks in SIGBench.</span>
</Figure>

## Experiment
We evaluate several top-tier MLLMs on SIGBench mainly from five modal families: 1) open-source models such as InternVL and Qwen-VL; 2) Proprietary models including OpenAI GPT, Google Gemini and Anthropic Claude. We conduct both SIG-based and MC(VQA)-based In-context Learning (ICL) on GPT-4o and Gemini-2.5-Pro.
### Results of Zero-shot Inference on SIGBench for Grid-based VSI tasks
<Figure>
  <Image slot="figure" source={zs_results_grid} altText="annotation_pipeline." />
  <span slot="caption">Quantitative comparison of different MLLMs on SIGBench for general VSI tasks. P, R, F1, and AssA means precision, recall, F1-score and Association Accuracy, respectively. S and WS means graph similarity and weighted graph similarity. Acc means accuracy. Dark blue and light blue indicates the best and the second best result among all models.</span>
</Figure>

### Results of SIG-based ICL using Random Sample Selection
<Figure>
  <Image slot="figure" source={icl_results_grid} altText="annotation_pipeline." />
  <span slot="caption">Quantitative comparison of 3-shot ICL for general VSI tasks on SIGBench-tiny. Z-S means zero-shot, ICL-MC meaning ICL using multiple-choice VQA and ICL-SIG meaning ICL using SIG. light red indicates the results that is worse than zero-shot after applying ICL on GPT-4o and Gemini-2.5-Pro.</span>
</Figure>

### Visualization of Grid-based VSI tasks Results
<Figure>
  <Image slot="figure" source={icl_vis_grid} altText="annotation_pipeline." />
  <span slot="caption">Visualization of SIG-empowered VSI results. SRD-Dir and SRD-Prox denotes the Acc in SRD (Directional) and SRF (Proximal). (a) demonstrate the performance of human and different models in grid-based tasks on SIGBench. Even for leading MLLMs, there is a substantial gap compared to human performance in VSI. (b) and (c) denotes the ICL results of GPT-4o and Gemini-2.5-pro on SIGBench-tiny. ICL-SIG outperforms the zero-shot baseline in all VSI metrics and delivers more general improvements than ICL-MC.</span>
</Figure>

### Results of Zero-shot Inference on SIGBench for Human-Like VSI tasks
<Figure>
  <Image slot="figure" source={human_like_results} altText="annotation_pipeline." />
  <span slot="caption">Quantitative comparison of different MLLMs on SIGBench dataset for human-like visual-spatial intelligence tasks. H means human-like and KL-D means KL-Divergence.</span>
</Figure>

## Acknowledgement
The author team would like to share the sincere thank to Wei Zhang from U.S. Department of Transportation (USDOT) for providing the valuable U.S. Federal Highway Administration driving dataset, based on which we construct our proposed benchmark. The author team also appreciate the valuable suggestions from Junyue Jiang, Linshen Liu and Yibo Zhao during the discussion about this project.


{/* ## Image comparison slider

An interactive, accessible slider component with keyboard navigation.
<Figure>
  <ImageComparison slot="figure" client:load imageUrlOne={dogsDiffc.src} imageUrlTwo={dogsTrue.src} altTextOne="Photo of two dogs running side-by-side in shallow water, lossily compressed using the DiffC algorithm" altTextTwo="Original photo of two dogs running side-by-side in shallow water" />
  <span slot="caption">A photo of two dogs running side-by-side in shallow water, lossily compressed using the <a href="https://jeremyiv.github.io/diffc-project-page/">DiffC algorithm</a>.</span>
</Figure>

## Two columns

Use the two columns component to display two columns of content. In this example, the first column contains a figure with a YouTube video and the second column contains a figure with a custom [React](https://react.dev/) component. By default, they display side by side, but if the screen is narrow enough (for example, on mobile), they're arranged vertically.

<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="wjZofJX0v4M" />
    <span slot="caption">Take a look at this YouTube video.</span>
  </Figure>
  <Figure slot="right">
    <Splat slot="figure" client:idle />
    <span slot="caption">Now look at this <a href="https://en.wikipedia.org/wiki/Gaussian_splatting">Gaussian splat</a>, rendered with a React component.</span>
  </Figure>
</TwoColumns>

## Heading levels

Use headings to divide your content into sections.

### Heading 3

Go down a level to heading 3...

#### Heading 4

...and down again to heading 4.

## LaTeX

You can also add LaTeX formulas, rendered during the build process using [KaTeX](https://katex.org/) so they're quick to load for visitors of your project page. You can write them inline, like this: <LaTeX inline formula="a^2 + b^2 = c^2" />. Or, you can write them as a block:

<LaTeX formula="\int_a^b f(x) dx" />

## Tables

You can add simple tables using [GitHub Flavored Markdown syntax](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/organizing-information-with-tables):

| Model | Accuracy | F1 score | Training time (hours) |
| :--- | :---: | :---: | :---: |
| BERT-base | 0.89 | 0.87 | 4.5 |
| RoBERTa-large | 0.92 | 0.91 | 7.2 |
| DistilBERT | 0.86 | 0.84 | 2.1 |
| XLNet | 0.90 | 0.89 | 6.8 |
*/}

## BibTeX citation

```bibtex
@inproceedings{wu2025towards,
  title = {Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study},
  author = {Guanlin Wu, Boyan Su, Yang Zhao, Pu Wang, Yichen Lin, Hao Frank Yang},
  booktitle={NeurIPS},
  year = {2025}
}
```